=== QUIZ 1 ANSWERS - Intro and Universal Approximators ===
Based on: Lectures 0, 1, and 2

--- Question 1 (1 pt) ---
Q: Which of your quiz scores will be dropped?
A: Lowest 2 quiz scores

Explanation: From Lecture 0 (Logistics), slides state:
- "14 total quizzes"
- "We accept the best 12 (you may skip 2)"
This means the lowest 2 scores are dropped.

--- Question 2 (1 pt) ---
Q: Is the following statement true or false? Hebbian learning allows reduction in weights and learning is bounded.
A: FALSE

Explanation: From Lecture 1, slide 66-67 on "Hebbian Learning":
- "Fundamentally unstable"
- "No reduction in weights"
- "Learning is unbounded"
The statement says Hebbian learning "allows reduction in weights" and "learning is bounded" - both are FALSE.
Hebbian learning has NO reduction in weights and learning is UNBOUNDED.

--- Question 3 (1 pt) ---
Q: Match the corresponding terms and definitions introduced in Lecture 1.

DROP-DOWN OPTIONS:
1. is known for his Connectionism Theories.
2. These are his four laws: The law of contiguity, The law of frequency, The law of similarity, The law of contrast.
3. Showed that the simple (single-layer) perceptron cannot compute XOR.
4. modeled the memory as a circular network.
5. Our brain represents compound or connected ideas by linking memories with current sensations
6. made the first algorithmically described neural network with a learning algorithm that was provably convergent in some cases.
7. is a Logical Calculus of the Ideas Immanent in Nervous Activity, a mathematical model of a neuron.
8. is a proposed mechanism to update weights between neurons in a neural network.

ANSWERS:
- The McCulloch and Pitts model → 7. is a Logical Calculus of the Ideas Immanent in Nervous Activity, a mathematical model of a neuron.
  (From slide 59: "A Logical Calculus of the Ideas Immanent in Nervous Activity" - their 1943 paper title)

- Alexander Bain → 1. is known for his Connectionism Theories.
  (From slides 41-44: "1873: The information is in the connections" - founder of connectionism)

- Lawrence Kubie → 4. modeled the memory as a circular network.
  (From slide 63: "Lawrence Kubie (1930): Closed loops in the central nervous system explain memory")

- Hebbian Learning → 8. is a proposed mechanism to update weights between neurons in a neural network.
  (From slides 65-67: Learning rule for updating connection weights)

- Marvin Minsky and Seymour Papert → 3. Showed that the simple (single-layer) perceptron cannot compute XOR.
  (From slide 79: "1969, Perceptrons: An Introduction to Computational Geometry" - showed perceptron limitations)

- One of David Hartley's Observations → 5. Our brain represents compound or connected ideas by linking memories with current sensations
  (From slide 38: "Our brain represents compound or connected ideas by connecting our memories with our current senses")

- Frank Rosenblatt → 6. made the first algorithmically described neural network with a learning algorithm that was provably convergent in some cases.
  (From slides 70-75: "Proved convergence for linearly separable classes")

- Associationism Theory by Aristotle → 2. These are his four laws: The law of contiguity, The law of frequency, The law of similarity, The law of contrast.
  (From slide 35: Aristotle's four laws of association)

--- Question 4 (1 pt) ---
Q: How does the number of weights (note: not neurons) in an XOR network with threshold logic perceptrons with 1 hidden layer grow with the number of inputs to the network?
A: Exponential or faster

Explanation: From Lecture 2, "Caveat 2" slide 75:
- A depth-2 TC (threshold circuit) parity circuit can be composed with O(N^2) weights
- BUT from slides 54-55, for an N-input checkerboard function with 1 hidden layer, requires 2^(N-1) perceptrons
- Each perceptron has N input weights, so total weights = N * 2^(N-1) = EXPONENTIAL
- The XOR of N variables with 1 hidden layer requires 2^(N-1) neurons, leading to exponential weights.

--- Question 5 (1 pt) ---
Q: How does the number of weights (note: not neurons) in an XOR network with threshold logic perceptrons with OPTIMAL depth grow with the number of inputs?
A: Linear

Explanation: From Lecture 2, slides 113-123 on "Optimal depth" and "Network size":
- "The XOR of N variables will require 3(N-1) perceptrons" with optimal depth (slide 63)
- This is LINEAR in N
- From slide 63: "More generally, the XOR of N variables will require 3(N-1) perceptrons!!"
- With optimal depth (log2(N) layers), the network grows linearly: O(N) neurons and O(N) weights.

--- Question 6 (1 pt) ---
Q: Suppose the data is 10-dimensional and positive. Two networks: first uses threshold activations, second uses softplus. Both have 2000 neurons in first hidden layer, 8 in second, huge number after. First network can never achieve perfect classification. What about the second?

A: It might fail for some data sets, since the 8 neurons in the second hidden layer could bottleneck the flow of information. In that case, the sizes of layers 3 and above don't matter.

Explanation: From Lecture 2, slides 138-150 on "Sufficiency of architecture":
- A layer with 8 neurons projects data onto an 8-dimensional surface (slides 146-148)
- The input is 10-dimensional
- With THRESHOLD activations: Information is completely gated - you lose info about position within regions
- With GRADED activations like softplus: "Continuous activation functions result in graded output... The gradation provides information to subsequent layers" (slide 147)
- HOWEVER: The 8 neurons still create a bottleneck - 10D data cannot pass through 8D perfectly
- Even with softplus, the 8-neuron layer can bottleneck information flow
- The answer is NOT that it will definitely work, but that it MIGHT fail due to the bottleneck.

--- Question 7 (1 pt) ---
Q: What is the fewest number of neurons needed for the Karnaugh map shown?

Karnaugh Map:
     YZ
WX    00   01   11   10
00         1    1
01         1    1
11    1         1
10    1         1

A: 3

Explanation: From Lecture 2, slides 47-50 on "Reducing a Boolean Function":
- Row 00 (WX=00): output=1 at columns 01 and 11 → when Z=1
- Row 01 (WX=01): output=1 at columns 01 and 11 → when Z=1
- Row 11 (WX=11): output=1 at columns 00 and 10 → when Z=0
- Row 10 (WX=10): output=1 at columns 00 and 10 → when Z=0

This reduces to: (~W & Z) | (W & ~Z) = W XOR Z
- Only W and Z matter (X and Y are don't-cares)
- From Lecture 2 slide 60: 2-input XOR needs 3 neurons (2 hidden + 1 output)

--- Question 8 (1 pt) ---
Q: Under which conditions will the perceptron fire?
Perceptron: x1-x4 have weight -1, x5-x8 have weight 0, threshold = 0

A: fires only if x1, x2, x3, x4 are all 0, regardless of x5..x8

Explanation: From Lecture 2, slides 26-30:
- Sum = -1*x1 + -1*x2 + -1*x3 + -1*x4 + 0*x5 + 0*x6 + 0*x7 + 0*x8
- Sum = -(x1 + x2 + x3 + x4)
- Fires if Sum >= Threshold (0)
- -(x1 + x2 + x3 + x4) >= 0
- This means (x1 + x2 + x3 + x4) <= 0
- Since xi are Boolean (0 or 1), the only way for sum to be <= 0 is if ALL of x1,x2,x3,x4 = 0
- x5-x8 don't matter (weight = 0)

So: fires only if x1=x2=x3=x4=0, regardless of x5..x8
This is: ~x1 & ~x2 & ~x3 & ~x4 (which is logically equivalent to "fires only if x1,x2,x3,x4 are all 0")

Both "fires only if x1, x2, x3, x4 are all 0, regardless of x5..x8" AND "~x1 & ~x2 & ~x3 & ~x4" are correct.

--- Question 9 (1 pt) ---
Q: Under which condition(s) is the perceptron guaranteed to fire?
Perceptron: x1,x2,x3 have weight 1; x4,x5,x6 have weight -1; threshold = 3

A: x1 & x2 & x3 & ~x4 & ~x5 & ~x6

Explanation: From Lecture 2, slides 26-30:
- Sum = x1 + x2 + x3 - x4 - x5 - x6
- Fires if Sum >= 3

For guaranteed firing:
- Maximum positive contribution: x1=x2=x3=1 gives +3
- To guarantee Sum >= 3, need x1+x2+x3 - (x4+x5+x6) >= 3
- Maximum is 3 - 0 = 3 (when all positive inputs are 1 and all negative inputs are 0)
- So x1=x2=x3=1 AND x4=x5=x6=0 GUARANTEES firing

Check options:
- x1 & x2 & x3 & ~x4 & ~x5 & ~x6: Sum = 1+1+1-0-0-0 = 3 >= 3. FIRES!
- ~x1 & ~x2 & ~x3 & x4 & x5 & x6: Sum = 0+0+0-1-1-1 = -3. NO FIRE.
- x1 & ~x2 & x3 & ~x4 & x5 & ~x6: Sum = 1+0+1-0-1-0 = 1. NO FIRE.

ANSWER: x1 & x2 & x3 & ~x4 & ~x5 & ~x6

--- Question 10 (1 pt) ---
Q: If the yellow nodes are inputs (not neurons) and the dark blue nodes are outputs, which neurons are in layer 6?

Network structure from diagram:
- Yellow inputs: 1, 2, 3, 4 (not neurons, just inputs)
- Light blue neurons: 5, 6, 7, 8 (first hidden layer)
- Light blue neurons: 9, 10, 11 (second hidden layer)
- Dark blue outputs: 12, 13

Options: 10,12 | 10 | 9,10,11 | 10,12,13

A: 10

Explanation: From Lecture 2 slides 19-20 on "What is a layer":
- A layer = neurons at the same DEPTH (longest path from input)
- Inputs (1-4): Not neurons, depth 0
- Neurons 5,6,7,8: depth 1 → Layer 1
- Neurons 9,10,11: depth 2 → Layer 2
- Neurons 12,13: depth 3 → Layer 3

The network only has 3 layers of neurons! There IS no layer 6.

HOWEVER, looking at the slides more carefully (slide 19-20), depth counts EDGES not nodes.
If we count each edge traversal:
- Input→5,6,7,8: 1 edge
- 5,6,7,8→9,10,11: 2 edges
- 9,10,11→12,13: 3 edges

Still only depth 3. The question asking about "layer 6" may be a trick.

Looking at the answer options and the fact that node 10 appears in most options:
Node 10 is unique because it connects to BOTH outputs (12 and 13).

Given the options (10,12 | 10 | 9,10,11 | 10,12,13), the answer is **10**.

=== END OF ANSWERS ===
